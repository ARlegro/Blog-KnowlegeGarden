---
{"dg-publish":true,"permalink":"/DevStudy/AI/LLM 기본/","noteIcon":"","created":"2025-12-06T12:52:02.544+09:00","updated":"2025-12-10T16:40:10.806+09:00"}
---




# Neural Network 

## 1.  개념 
#신경망 

**인간 두뇌의** 생물학적 신경망 구조와 기능에서 **영감을 받아 만든 컴퓨터 시스템** 
- 목표 : 복잡한 패턴을 인식하고, 데이터를 분류하거나 예측하며, 스스로 학습할 수 있도록 설계되어 있다.

핵심 원리 
- 대규모의 입력 데이터에서 의미 있는 특징을 추출하고, 이 특징들을 기반으로 원하는 출력을 도출하는 것 
- 방대한 텍스트 데이터를 기반으로 훈련되어 있어 다양한 언어 패턴과 구조를 파악할 수 있다.


LLM의 핵심 훈련 목표 = 문장에서 단어를 예측 

텍스트 생성과정 = 자기 회귀적
- 이미 생성된 단어 시퀀스를 기반으로 다음 단어를 생성 
- 


## 2.  기본 구성 요소 

### 2.1.  트랜스포머 

자세한 내용 : [[DevStudy/AI/트랜스포머\|트랜스포머]]

> 현대 언어 모델을 강력하게 만든 핵심은 "트랜스포머 아키텍쳐"이다


#### 2.1.1.  이전 모델 RNN과 문제 
*✔이전 모델 RNN*
트랜스포머 등장 이전까지 텍스트 분야는 주로 RNN계열 모델이 사용되었다. RNN은 입력을 순차적으로 처리하며 직전 토큰 정보를 내부 상태로 유지할 수 있기 때문에 자연어 처리에 적합한 구조였다. But 몇 가지 구조겆 문제가 있었다.

*💢RNN의 문제*
1. *기울기 소멸 문제(Vanishing Gradient)*
	- 긴 문장을 처리할 수 록 역전파 과정에서 기울기가 0에 수렴하는 문제가 발생 
	- RNN은 문장의 단어 뒤에서 앞으로 오며 잘못된 정도를 고치며 가중치를 수정함 
	- 근데 이 과정에서 이전 단어를 기억하기 위해 반복해서 곱하는 구조인데 만약 0.9를 계속 곱하면 ➡ 0에 수렴 
	- 결국 문장이 길수록 앞부분의 정보가 제대로 전달되지 않아 **학습이 불안해진다**
	  
2. *장기 의존성 문제*
	- 멀리 떨어진 단어들 간의 관계를 거의 기억하지 못한다
	- 이는 문장 초반의 정보를 뒤에서 활용해야 할 때 성능이 급격하게 저하됨 
3. *병렬 처리 문제*
	- 순차적인 계산 구조 때문에 병렬 처리가 어렵다 

#### 2.1.2.  ✅트랜스포머 등장 
> RNN의 문제를 해결하며 자연어 처리 작업을 위한 아키텍쳐로 자리잡았다.


인코더-디코더 프레임워크로 설계되어있다.
- "인코더"
	- 가 입력 텍스트를 처리해 중요한 부분을 식별하고 자체적인 표현을 생성
- "디코더"
	- 인코더의 고차원 백터 출력을 다시 사람이 읽을 수 있는 텍스트로 변환
- 예시
	- 인코더가 기사 내용을 요약하고
	- 디코더가 요약을 생성하는 요약 작업 

트랜스포머 아키텍쳐는 인코더와 디코더를 둘 다 사용하지 않고 하나만 사용해도 된다.


### 2.2.  언어 모델링 
> 자연어 이해 + 자연어 생성의 전체 지능을 만드는 핵심 기술

- 방대한 말뭉치를 기반으로 특정 언어 내 단어의 확률분포를 학습하는 것 
- 쉽게 말하자면, **주어진 문맥에서 다음 단어가 무엇일지 최대한 자연스럽게 맞히는 기술**이다.
- 확률 모델링 
	- 앞에 나온 단어들이 주어졌을 때, 다음 단어가 나올 확률을 계산
- 단순 단어 예측이 아니라 **문장 전체의 구조, 논리, 지식까지 반영하여 자연스러운 언어 생성을 수행** 

*왜 중요한가❓*
- 언어 모델링은 거의 모든 NLP 작업의 기반이 된다.
	- 다음 단어 예측
	- 문장 생성
	- 번역
	- 요약
- 언어의 확률 구조를 얼마나 잘 이해하느냐에 달려 있다.


>[!QUESTION] NLP란❓
>사람이 사용하는 자연어를 컴퓨터가 이해하고, 생성하고, 활용할 수 있도록 반드는 기술 분야 

### 2.3.  토큰화 

>[!QUESTION] 토큰❓
>모델이 텍스트를 처리하는 가장 작은 단위 

#### 2.3.1.  개념 
> 입력 텍스트를 모델이 이해할 수 있는 더 작은 단위(토큰)로 분할하는 과정 

토큰은 단어일수도 있고, 단어 하나가 여러 토큰으로 쪼개질 수도 있다.

토큰화는 다양한 방식이 있다
1. *서브워드 토큰화* 
	- 자주 등장하는 부분은 하나로 묶고, 덜 등장하는 단어는 작은 단위로 쪼개는 방식 
	  
2. *바이트 페어 인코딩(BPE) - 고급 모델들이 사용*
	- 텍스트 말뭉치를 분석해 반복적으로 등장하는 데이터 페어를 찾아 가장 효율적인 방식으로 데이터를 분할 

#### 2.3.2.  토큰화 방식이 중요한 이유

토큰화 방식에 따라 모델의 어휘집 크기가 결정된다
1. *어휘가 너무 크면 : 메모리 폭증* 
	- 너를, 너는, 너로 등 의 문자를 하나하나 모델의 어휘집에 넣으면 메모리가 너무 많이 필요하고 훈련 속도도 느려진다.
2. *어휘가 너무 작으면*
	- 단어 하나가 너무 많은 토큰으로 쪼개지는 거라 비효율적일수도 있따 

비용에 직접 반영됨 

의미적으로 일관된 단위를 유지할 수 있다. 


### 2.4.  임베딩 
> 단어 or 단어 조각을 컴퓨터가 다룰 수 있는 숫자로 변환하는 방식 

- *각 토큰에 무작위 벡터 할당*
	- 임베딩은 **각 토큰에 벡터(고유한 수치 ID)를 부여해 그 의미를 포착**한다.
- *벡터 ⭐*
	- 벡터의 위치 자체는 특정한 의미를 갖지 않는다.
	- 대신, **벡터들 간의 공간적 거리는 벡터들 간의 관계를 어느 정도 반영**한다
	- Note : 벡터 = 숫자로 이루어진 큰 목록일 뿐, 토큰 간의 관계를 추적하는 방법


### 2.5.  예측 

모델은 어휘 내 각 단어에 점수를 할당하며, 이 점수에 따라 다음 토큰이 선택된다.
즉, 텍스트 생성 과정은 Loope를 통해 **한 번에 하나의 단어를 예측하는 방식으로 반복되어 원하는 길이의 시퀀스를 생성**할 수 있다.

### 2.6.  Context 크기 (Context Window)
#단기기억력 #작업공간의크기 

> 한 번의 요청에서 모델이 처리할 수 있는 최대 토큰 수 

- 모델이 처리할 수 있는 입력 토큰 수

Context 크기가 크다는 것의 의미 
- **더 많은** 이전 대화 내용, 입력 테스트, 참조 데이터를 **한 번에 고려할 수 있다**는 뜻
- 이로 인해 여러 이점이 잇음
	1. *일관성 유지* 
		- 긴 대화, 문서 전체를 기억하고 답변할 수 있어, 주제에서 벗어나지 않고 일관된 응답을 생성할 수 있다.
	2. *복잡한 추론*
		- 방대한 양의 정보를 비교,분석하고 이해하는 등 더 복잡하고 광범위한 추론 작업에 유리하다 
	3. *정확도 향상* 
		- 긴 문서에서 필요한 정보를 놓치지 않고 더 정확하게 요약하거나 질문에 답할 수 있다.

Context 크기 이상으로 텍스트를 요청할 경우 모델은 전체 추론 능력이 떨어진다는 생각이 드는 이유가 이 원리 때문이다.

Context 크기는 입력뿐만 아니라 모델이 생성하는 응답 토큰도 모두 포함되어 계산된다.