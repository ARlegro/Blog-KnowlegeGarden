---
{"dg-publish":true,"permalink":"/DevStudy/AI/트랜스포머 기본(영상 정리)/","noteIcon":"","created":"2025-12-06T12:52:02.559+09:00","updated":"2025-12-14T13:01:23.774+09:00"}
---



참고 : https://www.youtube.com/watch?v=g38aoGttLhI

### 0.1.  목차
- [1.  트랜스 포머의 등장 배경과 진화](#1--%ED%8A%B8%EB%9E%9C%EC%8A%A4-%ED%8F%AC%EB%A8%B8%EC%9D%98-%EB%93%B1%EC%9E%A5-%EB%B0%B0%EA%B2%BD%EA%B3%BC-%EC%A7%84%ED%99%94)
- [2.  트랜스포머 안에서 데이터가 어떻게 이동하는지](#2--%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8-%EC%95%88%EC%97%90%EC%84%9C-%EB%8D%B0%EC%9D%B4%ED%84%B0%EA%B0%80-%EC%96%B4%EB%96%BB%EA%B2%8C-%EC%9D%B4%EB%8F%99%ED%95%98%EB%8A%94%EC%A7%80)
- [3.  머신러닝](#3--%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D)
- [4.  Self-Attention](#4--self-attention)


## 1.  트랜스 포머의 등장 배경과 진화 

### 1.1.  배경 
- 현대 언어 모델의 강력한 성능은 트랜스포머(Transformer) 아키텍쳐에 비롯함 
- 트랜스포머는 기존 RNN 계열 모델이 가지던 **순차 처리 한계와 장기 의존성 문제**를 해결하며, 현재 LLM의 사실상 표준 구조로 자리 잡았다.

### 1.2.  트랜스포머의 진화 

1. *(초기)인코더-디코더 아키텍쳐로 제시*
	- 목적 : 기계 번역 
2. *(2018) 인코더 전용 모델의 등장 - BERT*
	- 문장의 양방향 문맥 이해에 특화
	- 주로 분류, 질의응답, 의미 이해 테스크에 활용 
3. *디코더 전용 네트워크 도입(GPT 초기 버전)*
	- GPT 시리즈
	- 다음 **토큰 예측** 기반의 생성 모델
	- 대부분의 LLM이 이 구조를 채택 

> 인코더 전용 모델과 디코더 전용 모델은 설계 뿐만 아니라 **학습 목표도 다르고** 이에 따라 **모델의 작동 방식과 결과물에 큰 영향**을 준다.

### 1.3.  트랜스포머의 핵심 혁신 
> 고도의 병렬화

트랜스포머는 RNN처럼 토큰을 순차적으로 처리하지 않고, **문장 내 모든 토큰을 동시에 처리**할 수 있는 구조를 갖는다.
- GPU 병렬 연산에 최적화
- 대규모 데이터 학습 가능
- LLM 스케일링의 기반이 됨

### 1.4.  번외 : GPT란 

GPT = Generative Pre-Trained Transformer 
- **Generative**: 텍스트 생성
- **Pre-Trained**: 대규모 코퍼스로 사전 학습
- **Transformer**: 디코더 기반 트랜스포머 구조


GPT는 본질적으로 다음 문제를 푼다.<BR>
**“이전 토큰들이 주어졌을 때, 다음에 올 토큰의 확률 분포는?”**

즉, 문장을 이해하는 것이 아니라, **다음 토큰의 확률 분포를 예측하는 것이 목적**

### 1.5.  예측과 문장 생성의 차이

겉보기에는 “다음 단어 예측 = 문장 생성”처럼 보이지만,  
**좋은 예측이 곧 좋은 문장을 의미하지는 않는다.**

- GPT 초기 버전:
    - 문법적으로는 맞지만
    - 문맥적으로 부자연스러운 출력 다수


*문장 품질은 단순 예측이 아니라 아래의 요인에 의해 결정된다.*
- 문맥 표현력
- Attention 구조의 정교함
- 학습 데이터와 파라미터 규모  
  
## 2.  트랜스포머 안에서 데이터가 어떻게 이동하는지

### 2.1.  임베딩 (토큰 -> 벡터)

과정
1. 입력 문장은 토큰(Token)단위로 분해됨
2. 각 토큰은 고정 차원의 벡터로 변환됨

의미가 비슷한 단어들은 벡터공간에서 가까운 위치에 존재 

> 백터값은 처음에 랜덤이지만 학습을 통해 의미 기반으로 재배치된다.

### 2.2.  Attention 블록에서의 정보 교환 
각 토큰 벡터는 Attention 블록을 통과하면서 **다른 토큰들과 정보를 주고받으며 값이 갱신**된다.

이처럼 정보를 주고받는 연산 = Attention (토큰 간 관계를 동적으로 계산산)


![Pasted image 20251205133715.png](/img/user/supporter/image/Pasted%20image%2020251205133715.png)

### 2.3.  Transformer Layer 구조

트랜스포머는 아래 블록 중 여러 층을 반복하며 문장이 어떤 의미를 가지고 있는지 마지막 벡터에 담게 됨.
- Self-Attention
- Feed Forward Network(MLP)

![Pasted image 20251205133938.png](/img/user/supporter/image/Pasted%20image%2020251205133938.png)

마지막 벡터를 기준으로 특정한 연산을 수행해서 다음에 올 수 있는 모든 토큰(텍스트 조각)에 대한 확률 분포를 생성 
![Pasted image 20251205134037.png](/img/user/supporter/image/Pasted%20image%2020251205134037.png)

이런 식으로 어떤 텍스트 일부가 주어졌을 때 다음 단어를 예측할 수 있다면 반복해서 예측 

### 2.4.  왜 마지막 벡터만으로 다음 토큰을 예측?

>[!QUESTION] 
>앞의 수천개의 벡터는 문맥에 도움이 안되는가?

정답은 **효율성** 때문이다.
- 이미 Attention을 통해 마지막 벡터 안에 **이전 모든 토큰의 문맥 정보가 압축됨**
- 따라서 마지막 벡터만 사용해서 예측하면 계산량과 메모리 사용을 크게 줄일 수 있다.


## 3.  머신러닝 
머신러닝이란 데이터를 가지고 모델이 어떻게 동작해야하는지를 알아내는 방식 중 하나? 

수많은 Input-Output 쌍을 가지고 모델에 넣었을 때 모델 안에 있는 파라미터들을 조절하면서 성공적인 파라미터들을 찾아가는 과정 
![Pasted image 20251205140502.png](/img/user/supporter/image/Pasted%20image%2020251205140502.png)

입력/출력값 사이의 관계인 함수 자체를 알아가는 과정 

### 3.1.  가장 단순한 학습 - 선형 회귀
입력/출력이 단순 숫자인 경우 

![Pasted image 20251205140613.png](/img/user/supporter/image/Pasted%20image%2020251205140613.png)
기울기와 Y절편이라는 매개변수로 표현됨 

(현실 딥러닝 모델은 더 복잡 - GPT3는 1750억개의 매개변수가 있다)

딥러닝 모델들은 역전파 Backpropagation을 사용 



GPT 모델에서 대부분의 계산이 행렬 벡터의 곱으로 이루어져 있다.



## 4.  Self-Attention

>[!tip] 같은 단어가 문장에 따라 다른 의미를 갖는 이유 = Self-Attetion
>각 단어의 중요도를 문맥에 따라 동적으로 계산 


트랜스포머는 **셀프 어텐션** 메커니즘을 핵심으로 사용하여 **입력 시퀀스 내의 모든 단어 쌍 관계를 계산**하고, 이를 통해 **각 단어의 문맥적 의미를 파악**한다.

이것이 **같은 단어가 다른 문장에서 다른 의미를 가질 때** 트랜스포머가 **구분**할 수 있는 비결


---
### 4.1.  어텐션 메커니즘 - 문맥 이해의 핵심 
![Pasted image 20251205161344.png](/img/user/supporter/image/Pasted%20image%2020251205161344.png)
>[!QUESTION] 똑같은 단어가 다른 문장에서 다른 의미를 가진다는 것을 어떻게 트랜스포머가 알까❓
>각 단어에 대한 문맥적 가중치를 계산하기 때문 

트랜스포머는 
- ❌임베딩을 단순히 벡터로 사용하는 것이 아니다❌
- ✅해당 벡터를 **단어 사이의 관계를 동적으로 계산에 문맥에 맞게 재조정**하는데 방식으로 이루어짐.

이러한 동적으로 재조정을 수행하는 장치 = **"Self Attention"메커니즘**이다.

---
### 4.2.  전체 흐름 구조 
Self-Attention은 아래 3단계로 이루어진다:
1. *Input Embedding 생성*
2. *Q, K, V 벡터로의 투영*
3. *Attention Score → 가중치 적용 → 문맥 벡터 생성*

---
### 4.3.  Step 1 - 입력 단어를 임베딩 공간으로 변환 
![Pasted image 20251205162947.png](/img/user/supporter/image/Pasted%20image%2020251205162947.png)
- **입력 문장의 각 단어는 단순 임베딩 벡터로 변환**된다.
- 이 때, **위치 정보(Positional Encoding)까지 더해져** 단어의 순서 정보도 함께 포함된 **초기 입력 벡터가 생성**된다.
- 이 단계의 임베딩은 아직 문맥적 의미 ❌


---
### 4.4.  Step 2 - 셀프 어텐션을 위한 Q, K, V 생성 
각 단어 벡터는 셀프 어텐션 계산을 위해 세 가지 다른 벡터인 쿼리, 키, 벨류로 투영된다.

>[!QUESTION] 투영 = 임베딩 벡터를 각각 다른 목적에 맞게 변형한 새로운 벡터(Q, K, V)로 바꾸는 과정 
>단어 임베딩 하나만으로는
>- “어떤 단어에 주목해야 하나?”(Query)
>- “나의 정보적 특성은 무엇인가?”(Key)
>- “내 실제 의미 정보는 이거야.”(Value)
>
>이러한 3가지 역할을 동시에 수행하기 어렵기에 변환이 必


|벡터|역할|
|---|---|
|**Query (Q)**|"나는 어떤 단어를 봐야 하지?"를 묻는 역할|
|**Key (K)**|"나는 이런 정보를 가진 단어야."라고 대답하는 꼬리표 역할|
|**Value (V)**|단어의 실제 의미 정보(내용물)|

- Q, K 는 관계(유사도)를 게산하는 데 사용됨
- V는 최종 출력에서 의미를 전달하는 요소가 됨 

### 4.5.  Step 3 - 어텐션 계산 : 문맥 기반 의미 재구성 

Self-Attention은 아래 3단계로 동작한다.

1. *관계성 점수 계산*
	- 현재 단어의 Q와 다른 단어의 K를 내적하여 **각 단어 간 관련도를 계산** 
	- **점수가 크다 ➡ 현재 단어가 그 단어에 집중해야 한다**는 의미 
	  
2. *Softmax로 중요도 확률 계산*
	- 점수들을 Softmax에 통과시키면 **어텐션 가중치(중요도 분포)**가 만들어진다.
	- 모든 가중치 합은 1 
	- **각 단어가 얼마나 “주요하게” 고려되는지**를 수치로 표현
3. *가중치 x V -> 문맥 백터 생성* 
	- Softmax 가중치를 각 단어의 V에 곱해 모두 더하면 현재 단어에 대한 **문맥적 의미가 반영된 새로운 벡터**가 생성된다.
	- **의미적으로 중요한 단어들의 V는 더 크게 반영**됨
	- 따라서 같은 단어라도 문맥에 따라 **완전히 다른 표현**을 갖게 됨

이처럼, 트랜스포머는 Q,K,V라는 세 가지 관점을 통해 단어 간의 동적인 관계를 파악하고 같은 단어라도 문맥에 따라 매번 다른 문맥적 의미를 갖도록 스스로 조정함 
			  
### 4.6.  정리 - 문맥에 따라 의미가 달라지게 할 수 있는 이유 


트랜스포머는 단어를 하나의 벡터로 고정하지 않고,
- 주변 단어들과의 관계(Q·K·V)  
- Softmax 기반 중요도 계산  
- 중요한 단어의 V를 더 많이 반영

을 통해 **매 토큰을 문맥에 맞게 재계산한 동적 표현(문맥 벡터)** 으로 만들기 때문에 똑같은 단어라도 문장에 따라 다른 의미를 자연스럽게 이해할 수 있다

쿼리랑 키랑 헷갈리니까... 

| **특징**       | **쿼리 (Q, Query)**                                                | **키 (K, Key)**                                                      |
| ------------ | ---------------------------------------------------------------- | ------------------------------------------------------------------- |
| **역할**       | **정보를 찾는 질문 (탐색 주체)**                                            | **정보의 주제/주소 (탐색 대상)**                                               |
| **계산 목적**    | 이 단어가 다른 단어들과 **얼마나 연결되어야 하는지**를 결정                              | 다른 단어들이 $Q$에게 **얼마나 관련 정보를 제공할 수 있는지**를 결정                          |
| **가중치와의 관계** | **가중치 생성의 주체.** $Q$와 $K$의 유사도 계산으로 가중치($\text{Attention}$)가 생성됨. | **가중치 생성의 대상.** $Q$와 비교되어 그 단어의 **중요도($\text{Attention}$ 가중치)**를 결정 |
| **최종 가중치**   | **아닙니다.** $Q$ 자체는 최종 가중치(Attention Weight)가 아닙니다.                | **아닙니다.** $K$ 자체는 최종 가중치(Attention Weight)가 아닙니다.                   |

