---
{"dg-publish":true,"permalink":"/os/computer-system/week01/1-1/","noteIcon":"","created":"2025-07-11T22:37:22.926+09:00","updated":"2025-07-21T10:23:04.479+09:00"}
---




#비트 #컨텍스트 

### 비트란❓ 
> 정보의 최소 단위 
---
#### 개념 
- **데이터를 나타내는 최소 단위**
- 모든 데이터는 0과 1의 조합으로 구성되는데, 이 0또는 1이 하나의 비트가 됨. 
- 모든 것들을 디지털 신호로 바꾸는 과정에서, **데이터는 비트 단위로 나뉘어 0과 1로 저장되고 처리된다.**
- **소스 프로그램**은 0 or 1로 표시되는 **비트(bit)들의 연속**이며, 바이트(byte)라는 8비트 단위로 구성된다.


>[!EXAMPLE] 바이트(Byte) 
>- **각 바이트**(byte)는 **프로그램의 텍스트 문자**를 나타내며,
>- **프로그램은 연속된 바이트(Byte)들로 파일에 저장**된다.

---
#### ASCII 코드에서 bit
>[!tip] ASCII 코드 
>- 0과 1밖에 모르는 **컴퓨터가 영문 텍스트를 표현하기 위해** 가장 오래되고 널리 사용되는 **문자 인코딩 표준** 중 하나 

- ASCII코드는 **각 문자를 "바이트 길이의 정수값"으로** 나타낸다
	- ASCII는 각 문자 표현 시 **7bit**를 사용한다. (7bit ➡ 128가지 표현 가능) ❗
	
		| 문자  | 바이트(Byte) |
		| --- | --------- |
		| `#` | 35        |
		| `i` | 105       |
		| `n` | 110       |
		| `c` | 99        |
		|     |           |

>[!QUESTION] 컴퓨터에서 데이터  처리 기본단위 = 8bit 인데 왜 ASCII는 7bit로 표현❓
>- 7bit로 문자를 만들고 8bit 공간에 저장한다.
>- 남은 1bit 
>	1. 0으로 채워지거나
>	2. 패리티 비트로 사용 for 데이터 오류 검사 

---
### 컨텍스트란❓ - 비트에 의미를 부여

#비트해석 #비트규칙 

---
#### 1. 정의 
>정의 :  Bit 배열이 어떤 종류의 정보인지를 컴퓨터 시스템에 알려주는 '규칙' or '환경'
- **같은 비트 배열이라도 Context가 다르면 완전히 다른 정보를 의미** 
	ex. 똑같은 000001100 이 다르게 해석 
- 반대로 Context가 다를 때, 다른 비트배열이더라도 같은 정보를 의미할 수도 있음 

---
#### 2. 종류 
주요 컨텍스트의 종류 

1. **데이터 타입**
	- **컨텍스트가 다양** : 정수로 해석할 것인지❓ 부동소수점으로 해석할 것인지❓문자로 해석할 것인지? Boolean으로 해석할 것인지❓ex. 'A'로? 65로? 
	- **프로그램에서 타입 지정 이유도 컨텍스트 논리이다** : 비트 배열을 어떤 컨텍스트로 해석할지 컴파일러 or 인터프리터에게 명확히 알려주기 위해 

2. **인코딩**
	- ASCII? UTF-8?, ECU-KR 등 **어떤 문자 인코딩 표준을 따랐는지에** 따라 완전히 다른 문자로 해석될 수 있다(같은 비트 패턴이라도)
	  
3. **프로그램 명령어**
	- CPU가 특정 비트 배열을 **'명령어'로 해석하게 하는** 컨텍스트가 있다 
	- 즉, `+`가 문자일수도 명렬어일수도 잇는 것 
	  
4. **메모리 주소**
	- 비트들이 어떤 메모리에 저장되어 있는지에 따라 데이터가 정보가 달라진다.
	- 단순 프로그램 코드 변수의 값일수도 있고, OS 데이터의 일부일수도 있다.
	  
5. **프로그램 상태 및 흐름** 
	- "**어느 시점**에 **어떤 함수가 호출**되고 **어떤 변수가 활성화**되어 있는지"에 따라 **비트 해석도 달라진다**

---
### 결론 
1. 컴퓨터의 모든 것은 비트의 흐름이다
2. 비트는 원시적 데이터에 불과하다
3. 비트의 흐름에 의미를 부여하는 것이 컨텍스트이다. (해석 규칙과 환경)

