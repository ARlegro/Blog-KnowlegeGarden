---
{"dg-publish":true,"permalink":"/프로젝트/나만무/나만무 개인 구현 기능 정리/","noteIcon":"","created":"2025-12-09T17:20:11.022+09:00","updated":"2025-12-13T09:41:16.111+09:00"}
---


*Github* : [링크](https://github.com/NaManMu-10th-team7)


*관련 글* 
- [[프로젝트/나만무/나만무 정리\|나만무 정리]]
- [[프로젝트/나만무/AI-Agent/LangGraph 개념\|LangGraph 개념]]
- [[프로젝트/나만무/AI-Agent/AI-Agent 아키텍처 리팩토링 - Tool 어댑터 패턴과 후처리 노드 분리 패턴\|AI-Agent 아키텍처 리팩토링 - Tool 어댑터 패턴과 후처리 노드 분리 패턴]]
- [[프로젝트/나만무/AI-Agent/Prompt Caching 최적화 결과\|Prompt Caching 최적화 결과]]
- [[프로젝트/나만무/실시간 편집/나만무 Redis 활용 (Cache)\|나만무 Redis 활용 (Cache)]], [[프로젝트/나만무/실시간 편집/나만무 Redis 활용 (adapter)\|나만무 Redis 활용 (adapter)]], [[프로젝트/나만무/AI-Agent/AI Agent 타임아웃 - 공간 인덱싱 적용\|AI Agent 타임아웃 - 공간 인덱싱 적용]]

| 분야  | 기술 스택                                               |
| --- | --------------------------------------------------- |
| 백엔드 | NestJS, TypeORM, Python, FastAPI                    |
| AI  | LangChain, LangGraph, AWS Bedrock, Claude 4.5 Haiku |
| DB  | PostgreSQL, pgvector,  Elastic Cache(Redis)         |
| 인프라 | AWS EC2, AWS S3, AWS MQ, RDS, ECS Fargate           |

---
## 1.  개인 구현 기능 요약

1. AI Agent 구현 - LangGraph 기반 대화형 여행 계획 도우미
2. Prompt Caching 적용을 통한 비용 최적화 구조 구현 
3. 소켓 통신 -Redis I/O Adapter, Cache 기반 실시간 협업 시스템
4. 공간 검색 최적화
5. 장소 데이터 수집/가공 파이프라인 구축 후 배포 
6. Rabbit MQ를 통한 비동기 행동 추적을 통해 개인화 추천 및 랭킹에 반영 
7. DB 설계/관리, 프로젝트 세팅 : [ERD 링크](https://www.erdcloud.com/d/vZioi9856wurCMtAq)
8. OR-Tools의 TSP 솔버를 통한 여행 경로 최적화 기능 


---

## 2.  AI Agent 구현 - LangGraph 기반 여행 기반 도우미 

### 2.1.  프로젝트 요구사항 및 구현 목적
여행 추천뿐 아니라 일정 조정, 장소 교체, 일정 재구성, 분석 등 복합적 행동을 하나의 대화로 처리해주는 AI Agent가 필요. <br>
또한 사용자의 질문은 종종 모호하며, 여러 턴에 걸쳐 복합적인 작업을 요구했다. 이를 만족하기 위해서는 다음과 같은 능력이 필요했다.
1. *지속적인 대화 맥락 이해*
	- 대화/채팅방의 맥락을 이해하고 기본 LLM부터 각종 도구를 활용해 **장소 추천 → 일정 생성/수정 → 분석**까지 이어지는 워크플로우 수행
2. *Tool Orchestration 필요*
	- 여러 도구들을 의도에 맞게 선택/조합/실행하는 구조 필요
3. *세션별 상태 유지 및 이전 결과 활용*
	- 여러 장소 추천받은 뒤 "XX는 별론데 이거말고 다른 곳 추천해줘"라고 하면 **이전에 추천됐던 다른 장소들도 제외하고 대체하려는 장소의 주변 장소를 추천**받기
	  
4. *비정형 질의 및 에러에 대한 내성*
	- 사용자가 준 입력뿐만 아니라 LLM이 생성한 잘못된 Tool 인자 자동 복구 필요
---
### 2.2.  문제 정의 및 고민했던 점

1. *대화 상태 관리 문제*
	- LangChain Memory는 메시지를 단순 append하는 형태라 **깊은 JSON 구조 파싱이 불안정**했고, 일정·장소 정보가 복잡해질수록 LLM이 이전 데이터를 정확히 재활용하지 못했다.
	  
2. *의도 분류 부재로 인해 흐름 제어 실패*
	- 새로운 검색인지? 기존 추천에서 제외하고 다시 추천인지? **같은 의도 분류가 없어서 불필요한 토큰 낭비와 잦은 오류가 발생**했다.
	  
3. *자동 에러 처리 & 복구*
	- LLM이 생성한 Tool의 인자값들이 타입 미스, 누락 등의 불가능한 요청으로 전달되는 경우가 종종 발생 ➡ 전체 Agent흐름이 즉시 중단되어 사용자 경험화
	- **상태 기반 대화형 Agent**를 목표로 했기에 에러에 대한 내성이 필수
	  
4. *토큰/비용 문제*
	- 시스템 프롬프트 + 도구 설명이 길어 **턴당 평균 34,000 tokens**가 발생해 비용 우려
	- 복잡한 대화일수록 비용이 폭증하는 구조

---
### 2.3.  LangGraph 기반 해결 전략 

#### 2.3.1.  아키텍처 개요 
> 구조화된 State하에서 **Router → Agent → Tools → Post-Processor → Agent 재진입**으로 이어지는 순환형 상태 머신을 설계하고 CheckPointer로 모든 state를 자동 관리

![Pasted image 20251210001508.png](/img/user/supporter/image/Pasted%20image%2020251210001508.png)

---
#### 2.3.2.  구조적 설계 및 효과 
![별론데사진모음 1.png](/img/user/supporter/image/%EB%B3%84%EB%A1%A0%EB%8D%B0%EC%82%AC%EC%A7%84%EB%AA%A8%EC%9D%8C%201.png)
1. *구조화된 State로 관리*
	- 복잡한 JSON을 LLM이 직접 파싱하도록 맡기는 것이 아니라, **명확한 필드로 구조화된 State**를 사용했다.
		```python
		class AgentState(TypedDict, total=False):
		    """LangGraph 상태 타입"""
		    messages: Annotated[Sequence[BaseMessage], operator.add]
		    session_id: str
		    intent: Literal["NEW_SEARCH", "REFINEMENT", "CONVERSATION", "FOLLOW_UP"] | None
		    last_recommended_places: List[SimplePlace]
		    excluded_place_ids: List[str]  # 제외할 장소 ID 리스트 (도구 호출 시 사용)
		```
	- *효과* 💙
		- LLM의 파싱 오류 없이 직접 state를 참조
		- 대화가 길어지고 복잡해져도 안정적으로 작동 
	  
2. *자동 재시도 기반 self-healing 흐름 확보*
	- 기존 : Tool 인자 오류 발생 → 즉시 중단.
	- 개선 : *Tool 실패 후 LLM이 잘못된 인자를 스스로 고쳐 재시도하는 self-healing 구조*
		1. Tool 호출 실패 
		2. 실패 정보가 Agent 노드로 전달 
		3. LLM이 필드를 자동으로 수정
		4. Tool 자동 재시도 
	- *실제 자동 복구 사례*
		- `string` → `list[UUID]` 타입 변경
		- 누락된 필드 자동 보완
		- 잘못된 장소 ID 자동 재매핑
	- *효과*💙
		- **실패율 감소** 및 복잡한 **multi-step 요청도 중단 없이** 진행 ➡ 사용자 경험 향상

3. *CheckPointer 기반 세션 상태 자동 저장* 
	- thread_id 기반 CheckPointer로 각 Workspace마다 **세션별 대화 히스토리 자동 관리**
	- 중간 노드에서 언제든지 복구가 되는 구조 
	- *효과*💙
		- 채팅방을 나가도 이어말하기 가 완벽하게 지원되는 multi-turn 경험 제공 
		- Graph 진행 중 오류가 나도 중간 노드 상태로 자동 복구 시스템 
	  
4. *Intent Router로 선택적 히스토리 전달 & 토큰 최적화*
	- 첫 사용자의 입력 처리의 의도 분류를 Router Node에서 수행
	- 의도에 따른 선택적 히스토리 전달
		- 새 검색 요청 → 최소한의 메시지만 전달
		- 이전 추천 수정/제외 요청 → 관련된 state만 전달    
		- 단순 대화 → 맥락 최소 전달
	- *효과*💙
		- 이를 통해 맥락은 유지하면서도 **불필요한 맥락과 토큰 낭비를 줄였다**
		  
5. *Tool → Post-Processor 리팩토링*
	- AI-Agent아키텍쳐를 재설계해 비즈니스 로직과 LLM 의존성을 분리하고, Tool별 전용 후처리 노드와 Update노드의 책임 분리로 유지보수성을 크게 높였습니다.
	- 배경, 방법,성과 등 자세한 내용 : [[프로젝트/나만무/AI-Agent/AI-Agent 아키텍처 리팩토링 - Tool 어댑터 패턴과 후처리 노드 분리 패턴\|AI-Agent 아키텍처 리팩토링 - Tool 어댑터 패턴과 후처리 노드 분리 패턴]]



---
## 3.  Prompt Caching 적용 - 비용 최적화 구조 구축
![프롬프트 캐싱 3.png](/img/user/supporter/image/%ED%94%84%EB%A1%AC%ED%94%84%ED%8A%B8%20%EC%BA%90%EC%8B%B1%203.png)
자세한 내용 : [[프로젝트/나만무/AI-Agent/Prompt Caching 최적화 결과\|Prompt Caching 최적화 결과]]
> [!WARNING] 이 내용은 별도의 별도의 test branch에만 적용 
> - 프로젝트가 끝난 뒤 개선한 내용이라 별도의 branch에 적용
> - [깃허브 링크](https://github.com/NaManMu-10th-team7/matetrip-ai)

---
### 3.1.  Preview - 과정 및 성과 요약 

*✔최적화*
- System Prompt(26k tokens)을 LLM KV Cache에 고정
- Router ~ Agent 전체 호출이 동일 캐시를 사용

*🧾결과*
- **평균 Input token 76% 감소 / 턴당 평균 비용 68% 절감**
- 장기 대화에서 Cache Hit 100% 유지 
- **비용 최적화 구조** 완성

---
### 3.2.  문제 정의 - Multi 노드 구조상 엄청난 비효율 
LangGraph로 상태 기반 대화를 수행 시 **26k tokens 규모의 시스템 프롬프트**와 **다수 Tool 설명**이 매턴마다 LLM으로 전송되는 구조. <br>
여러 턴에 걸쳐 대화를 하고 한 턴 안에 LLM이 여러 번 호출되는 구조라 **시스템 프롬프트가 중복 전송되는 문제** 발생
- **턴당 평균 34,000 tokens** 입력
- Router/Agent/Tool 각각에서 동일한 거대 프롬프트를 재전송 → 불필요한 비용

---
### 3.3.  해결 - AWS Bedrock Prompt Caching전략 사용
![프롬프트 캐싱2.png](/img/user/supporter/image/%ED%94%84%EB%A1%AC%ED%94%84%ED%8A%B8%20%EC%BA%90%EC%8B%B12.png)
- *어떤 구조❓*
	- Bedrock의 Prompt Caching을 활용해 **거대한 시스템 프롬프트를 최초 1회만 모델에 전송하고**, 이후에는 모델이 **KV Cache를 그대로 재사용하도록** 구조 전환
- *요청 별 흐름*
	- **첫 요청:** 시스템 프롬프트를 캐시로 저장(cache write)
	- **이후 요청:** 프롬프트를 토큰화/파싱하지 않고 cache read로 대체 
- 이로 인해, LLM의 토크나이징/임베딩/Attention 계산을 완전히 생략하는 구조적 최적화
```python
# AWS Bedrock Prompt Caching 활성화
cached_system_message = SystemMessage(
    content=[
        {"type": "text", "text": system_prompt},
        {"cachePoint": {"type": "default"}}
    ]
)

return ChatPromptTemplate.from_messages(
		[
				cached_system_message, # cache 활용 
				MessagesPlaceholder(variable_name="chat_history", optional=True),
				("system", "User's workspace_id: {session_id}"),
				....
```

---
### 3.4.  🧾성과

| 지표                    | 개선 전    | 개선 후    | 개선율        |
| --------------------- | ------- | ------- | ---------- |
| **평균 Input Tokens/턴** | ~34,000 | ~8,000  | **76% 절감** |
| **턴당 비용**             | $0.0087 | $0.0028 | **68% 절감** |
| **Cache Hit율**        | 0%      | 100%    | 완전 고정      |
- **26k tokens 시스템 프롬프트가 비용 계산에서 사실상 제외됨**
- Router/Agent/Tool 전 구간에서 동일 KV 세그먼트를 재활용 → 100% Cache Hit
- 장기 세션에서 비용이 증가하지 않는 **비용 고정형 운영 모델로 전환**


---
## 4.  소켓 통신 - Redis I/O Adapter, Cache 기반 실시간 협업 시스템
![실시간 협업.png](/img/user/supporter/image/%EC%8B%A4%EC%8B%9C%EA%B0%84%20%ED%98%91%EC%97%85.png)
### 4.1.  프로젝트 요구사항 및 구현 목적

여행 일정을 여러 사용자가 **동시에** 편집할 수 있는 기능이 필요. 단일 서버가 아닌 **멀티 서버 환경**에서도 아래 기능들이 정확히 동기화 필요
- 실시간 채팅
- 지도 POI 마킹/해제
- 일정 추가/삭제/순서 변경
- 커서 이동, 지도 클릭 등 상호작용
- AI서버에서 계산한 일정 결과 실시간 반영

이 모든 이벤트가 **분산 서버 간 지연 없이 공유**되어야 했고, 사용자 입장 시 필요한 상태 또한 빠르게 로드되어야 했다

---
### 4.2.  고민했던 점

1. *멀티 서버 환경에서 이벤트 전파* 
	- 단순 Socket.IO로는 서버 간 이벤트가 공유되지 않아 서버 A의 유저가 보낸 이벤트를 서버 B의 유저가 받을 수 없음.
	- 이로 인해 중간 Message Broker역할을 하는 Redis I/O Adapter 도입
	  
2. *워크스페이스별 이벤트 격리*
3. *초기 상태 로딩 및 잦은 수정 이벤트로 인한 비용 증가*
	- 초기 상태를 **로딩할 때 매번 DB에서 가져오는 비용**과 잦은 수정으로 인해 **쓰기 증폭 문제** 고려
4. *POI 간 연결(Connection) 테이블 기반 순서 관리의 복잡도와 성능 문제*
	- 여행 일정 편집 기능은 Drag & Drop을 기반으로 하므로 **순서 변경이 매우 빈번**하다. 기존 구조에서는 POI 간 연속된 관계(prev, next)를 모두 Connection 테이블에 저장했기 때문에, POI가 조금만 움직여도 여러 Connection을 동시에 수정해야 했다.<br>![커넥션.png](/img/user/supporter/image/%EC%BB%A4%EB%84%A5%EC%85%98.png)

---
### 4.3.  해결 전략




#### 4.3.1.  Redis Pub/Sub 기반 Socket.IO Adapter 구축
멀티 서버 환경 전제를 두고  **Redis Adapter**로 서버 간 메시지를 연결하는 구조로 설계

어떤 서버에서 이벤트를 발생시켜도 Redis가 Pub/Sub으로 다른 서버의 Socket.IO에 자동 전달

**✅결과 : 스케일아웃 가능 구조 확보** - 서버 개수와 무관하게 단일 서버처럼 동작하는 실시간 통신 레이어 구축 

#### 4.3.2.  워크 스페이스 단위 격리

POI, Chat, Cursor 등 모든 이벤트는 워크스페이스별 Room으로 분리.
- Room 예시:
    - `poi:{workspaceId}`
    - `chat:{workspaceId}`
    - `schedule:{dayId}`
- 사용자는 자신의 워크스페이스 room만 구독 → **절대 엉뚱한 이벤트 수신 X**
- 다중 Room 구독도 자연스럽게 처리되도록 Gateway 계층에서 자동화

결과: **완벽한 이벤트 격리로 워크스페이스 간 갑섭 0건으로 만듬**

#### 4.3.3.  Redis Cache 
[[프로젝트/나만무/실시간 편집/Redis Cache 초기 로딩 성능 테스트\|Redis Cache 초기 로딩 성능 테스트]]

> 초기 로딩 및 상태 동기화 성능을 위해 Redis Cache도입

- DB(영속 저장용)
	- 단기 캐시 미스 시만 조회
	- 캐싱되면 재사용

초기 성능 테스트 결과 

| 측정 항목        | DB 직접 조회 | Redis 캐시 조회 | 개선율          |
| ------------ | -------- | ----------- | ------------ |
| **평균 응답 시간** | 4.57ms   | 0.20ms      | **95.5% 단축** |
| **표준편차**     | 0.76ms   | 0.11ms      | 85.5% 감소     |
| **최소 시간**    | 4.13ms   | 0.09ms      | 97.8% 단축     |
| **최대 시간**    | 7.78ms   | 0.50ms      | 93.6% 단축     |
Redis Cache 도입으로 **초기 로딩 시간을 약 95.5% 단축**하여 사용자에게 **즉각적인 응답**을 제공할 수 있게 되었습니다.


#### 4.3.4.  POI 커넥션 관련 설계 변경
[[프로젝트/나만무/실시간 편집/설계 변경 – Connection 기반 → Redis List 기반 순서 관리로 전환\|설계 변경 – Connection 기반 → Redis List 기반 순서 관리로 전환]]

> 커넥션 테이블 삭제 + Redis List + Hash 조합으로 일정 순서를 관리 전면 교체 


![hash-list.png](/img/user/supporter/image/hash-list.png)
poi_connection 테이블 삭제 후 Poi 테이블에 상태(Marked, Scheduled)필드와 순서 필드 추가
- 실시간 환경에서는 **Redis List + Hash 조합으로 일정 순서 및 POI를 관리**하도록 아키텍처를 전면 교체<br>![hash-list.png](/img/user/supporter/image/hash-list.png)
- *Redis List - 자료 여행 일정별 POI 순서 관리*
	- 각 plan_day의 POI 순서를 배열 형태로 유지
	- Drag & Drop → 프론트가 보내는 poi_ids 배열을 그대로 Redis List에 재구성  
	- ex. 일정 순서 → `List(workspace:{dayId}:scheduled)`
	
- *POI Hash - POI 상세 정보 관리*
	- Workspace별 POI 상세 정보 관리
	- 상태 및 sequence 필드는 List 인덱스와 동기화 ➡DB I/O 감소
	- ex. 장소 마킹 상태 → `Hash(workspace:{id}:poi)`

### 4.4.  성과 정리
1. **성능 개선 JOIN 불필요 + O(1)** : 업데이트할 때마다 JOIN을 활용할 필요가 없어짐, 순서 변경 시 거의 0(1) 수준
2. **단순성 및 유지보수성 향상** : POI 순서 관리 로직이 극적으로 단순
3. **쓰기 증폭을 방지**하고 **초기 로딩 시간을 약 95.5% 단축** : 평균 4.57ms → 0.2ms
4. **분산 환경 실시간 동기화 달성**

---

## 5.  공간 검색 최적화 
![공간검색최적화.png](/img/user/supporter/image/%EA%B3%B5%EA%B0%84%EA%B2%80%EC%83%89%EC%B5%9C%EC%A0%81%ED%99%94.png)
### 5.1.  Preview - 성과 요약 

- Haversine 기반 Full Scan 구조를 **PostGIS GiST 공간 인덱스 구조로 리팩토링**하여, 반경 검색 **성능을 최대 181배 개선**하고, AI Agent 타임아웃 문제를 제거했습니다.  
- 또한 **뷰포트 조회 최적화**로, 지도 줌/드래그 시 응답 시간을 대부분 **20~80ms 구간에 안정화**시켰습니다. #지연분산안정화

---
### 5.2.  프로젝트 요구사항 및 구현 목적

전국 단위의 여행지 데이터를 기반으로 다음 기능을 제공해야 했다.
1. 특정 위치 기준 **반경 N km 이내 검색**
2. 지도 드래그·줌 이동 시 즉시 결과를 갱신하는 **뷰포트(Focus) 조회**
3. 임베딩 유사도 검색과 거리 계산을 동시에 수행해야 하는 복합 검색

초기 구현은 곡률을 반영하는 Haversine 공식을 이용한 애플리케이션 레벨 거리 계산 구조였는데, 아래와 같은 문제가 발생했다.

| **구분**    | **초기 구조 (Haversine + Full Scan)**       | **문제점**                                    |
| --------- | --------------------------------------- | ------------------------------------------ |
| **거리 계산** | 애플리케이션 레벨에서 **Haversine 공식** 사용         | **Full Table Scan** 유발, 데이터 증가에 비례하여 성능 저하 |
| **성능 영향** | AI Agent가 툴을 재시도하며 **응답 시간이 수십 초까지 증가** | 심각한 **타임아웃** 발생, LLM 응답 오류 유발              |

결국 거리 계산/필터링을 애플리케이션에서 처리하는 방식으로는 AI Agent와 결합 시 절대 버틸 수 없다고 판단. **검색 구조 자체를 DB 공간 인덱스 기반으로 재설계**해야 했다.

---
### 5.3.  개선 전략 - PostGIS + GIST 공간 인덱싱
![공간검색.png](/img/user/supporter/image/%EA%B3%B5%EA%B0%84%EA%B2%80%EC%83%89.png)
>[!tip] 핵심 전략 
>1. 거리 계산과 필터링 로직을 애플리케이션에서 DB로 이전하고,  
>2. GiST 인덱스 기반 공간 검색으로 전환하여 검색 범위를 대폭 축소.

- GiST 공간 인덱스
- `ST_DWithin` 기반 반경 검색
- `ST_Intersects + ST_MakeEnvelope` 기반 지도 뷰포트 검색

참고 : [[프로젝트/나만무/DB관련/PostGIS와 GiST 인덱싱\|PostGIS와 GiST 인덱싱]]

### 5.4.  개선 효과 

#### 5.4.1.  반경 검색 성능 비교

| 케이스                  | PostGIS (GiST 인덱스) | Haversine (인덱스 없음) | 성능 차이         |
| ----------------------- | --------------------- | ----------------------- | ----------------- |
| 강남역 5km / 10개 조회  | 0.37초                | 16.47초                 | **약 44배 빠름**  |
| 강남역 10km / 20개 조회 | 0.94초                | 13.11초                 | **약 14배 빠름**  |
| 해운대 5km / 10개 조회  | 0.07초                | 12.44초                 | **약 181배 빠름** |
**결과:** 기존 **15초 이상** 걸리던 쿼리가 **0.07~0.94초** 수준으로 단축되어 **AI Agent 타임아웃 문제가 완전히 해결**되었습니다.

#### 5.4.2.  지도 뷰포트(Zoom/Drag) 조회 성능 안정화

**`ST_Intersects`** 및 **`ST_MakeEnvelope`** 함수를 GiST 인덱스와 함께 사용하도록 최적화


| **구분**     | **기존 방식 (Latitude/Longitude BETWEEN 사용)** | **개선 방식 (ST_Intersects + GiST 인덱스)**    |
| ---------- | ----------------------------------------- | --------------------------------------- |
| **성능 편차**  | 뷰포트가 클 때 **900ms까지 튀는 스파이크** 발생           | **대부분 20~80ms** 구간에 안정화                 |
| **사용자 체감** | 성능 편차가 커서 예측 불가능하고 불안정                    | **평균 응답 및 분산이 크게 줄어** 일관되고 예측 가능한 성능 확보 |


```SQL
-- 기존: Latitude / Longitude BETWEEN
WHERE latitude  BETWEEN :south AND :north
  AND longitude BETWEEN :west  AND :east;

-- 개선: PostGIS ST_Intersects + ST_MakeEnvelope + GiST 인덱스
WHERE ST_Intersects(
  p.geom,
  ST_MakeEnvelope(:west, :south, :east, :north, 4326)
);
```

*뷰포트(Focus) 기반 조회 성능 - Between vs ST_Intersects*
- 지도 Focus 이벤트마다 
- **기존 Between 방식**
    - 작은 범위 / 기존 조회 근처: 20~30ms 수준으로 그럭저럭 양호
    - 뷰포트를 크게 움직이거나, 완전히 다른 지역으로 점프, 아주 넓은 영역 조회 시 **수백 ms ~ 900ms까지 튀는 스파이크** 관찰
    - 성능 편차가 크고 예측 가능성이 낮아지는 문제가 존재.
        
- **PostGIS ST_Intersects + GiST 인덱스**
    - 대부분 케이스: **20~80ms 사이**에 응답
    - 좁은 뷰포트에서는 **Between과 거의 동일한 속도(20~40ms)**
    - 근처로 살짝 이동하는 일반적인 줌/드래그 상황에서는 **거의 항상 20~40ms 선에 수렴**
    - 아주 넓은 범위를 한 번에 조회하는 극단 케이스에서만 수백 ms 구간이 일부 존재하나, Between처럼 900ms까지 폭발하는 케이스가 크게 줄음
    - **평균 응답 및 분산이 크게 줄어 체감 성능이 안정화**

### 5.5.  결과 요약 
결과 : 공간 인덱스 도입으로 **평균 응답 속도**뿐만 아니라 **지연 분산(Latency Variance)까지 최소화**하여 반복적인 줌/드래그 환경에서 **안정적인 사용자 경험**을 제공할 수 있게 되었습니다.


---
## 6.  그 외 (간단히)

---
### 6.1.  장소 데이터 수집/가공 파이프라인 구축 후 Fargate 배포

프로젝트의 단순성을 탈피하기 위해 **커스텀 된 장소 데이터가 필요**
- 장소에 대한 정확한 사진
- 장소마다의 전체적인 리뷰
- 장소 임베딩 처리 - 개인화 추천을 위해
- 그 외 기본 장소 정보들 

처음에는, Kakao Local API를 통해 좌표들을 여러 기준으로 나눠서 어느정도 전국 데이터를 모으자는 주의 였다. 대충 해봤는데 20만개 데이터 정도가 모였다. 이를 가공 파이프라인에 넣었다.

(아래는 구축한 파이프라인)
![Pasted image 20251210152356.png](/img/user/supporter/image/Pasted%20image%2020251210152356.png)

*💢문제*
1. *절대적인 시간의 한계*
	- 가공 파이프라인은 아래와 같은 구조로 되어있으며 하나의 장소를 완전히 처리하는데 50초 정도가 걸렸다. 절대 2~3주라는 시간 안에 가공을 못할 것이다.
2. *여행객들이 좋아할만한 장소가 아니다*
	- 전국의 20만개의 장소 데이터들을 보는데, 특정 지역의 평범한 분식집-고깃집 등 그러한 장소들이 모이다보니 유의미한 여행 장소 데이터로는 비춰지지 않았다.
	  
3. *잦은 오류로 인한 서버 다운*
	- API 호출이 많다 보니 오류가 자주나서 멈췄다.
	- 로컬에서 돌리면 프로세스가 멈추고 직접 모니터링해야 했음. 계속해서 신경쓸 수는 없었기에 자동 복구 흐름이 필요
	  
4. *리소스 과다*
	- 파이프라인 자체가 이미지 파싱 / 리뷰 분석 / 임베딩 생성은 CPU·RAM을 많이 사용
	- 이로 인해 개발 환경 자체가 마비됨

*✔해결* 
1. *장소 압축* 
	- 한국 관광공사에서 제공하는 TourAPI의 데이터 선택
	- 관광객들을 상대로 하는 데이터다보니 서비스에 유의미한 데이터라고 판단
	- 카테고리 다양성을 위해 인기있는 장소들을 우선순위로 파이프라인에 넣음
	  
2. *ECS Fargate에 배포* 
	- 워킹 작업을 단순 EC2가 아니라 Fargate에 배포하여 수행했다.
	- 여러 CPU, 메모리 조합을 테스트해보면서 체감상 빠르고/적당하다고 느끼는 조합으로 튜닝
	- 결과
		- 오류로 멈춰도 자동 재시작되므로 **전체 파이프라인이 절대 멈추지 않음**
		- 로컬 리소스를 아끼면서 프로젝트의 다른 작업을 할 수 있었다.

*❗한계 - 컴퓨터가 좋은게 중요하지 않았다*
- 처음에는 컴퓨터 좋은 거를 써서 대규모 병렬 처리로 하면 빠르게 모을 수 있지 않을까 했다.
- 하지만 실제로 해봤는데 별 차이가 없었다.
- 왜냐면 CPU작업은 일정 부분 이미 CRAW4AI 라이브러리에서 병렬처리해주고 URL/리뷰 크롤링을 위한 I/O작업이 병목의 문제였는데, 그건 한번에 너무 많이 하면 Naver에서 막아놔서... 빠르게 할 수가 없었다(우회로직을 도입한 상황인데도 한계가 있었다)

더 좋은 방법이 있을 수 있겠지만 이 프로젝트 자체가 데이터를 모으고 가공하는 것이 **주 목적이 아니다보니** 더 이상 시간을 쏟고 싶지는 않았다. 이 작업을 제대로 구축하고 테스트하는데만 5일이 걸렸으며, 프로젝트의 어려운 부분을 도맡고 리드를 해야 하는 입장으로서 이 시간이 너무 비효율적이었기에 이 정도로만 하고 마쳤다.

---
### 6.2.  DB 설계, 관리, 프로젝트 환경 세팅 

> [ERD 설계도](https://www.erdcloud.com/d/vZioi9856wurCMtAq)

![나만무 7팀  (4).png](/img/user/supporter/image/%EB%82%98%EB%A7%8C%EB%AC%B4%207%ED%8C%80%20%20(4).png)



웹개발 경험이 없는 팀원이 4명과 함께 진행되다보니 백엔드/워커서버 환경 구성 및 DB 설계/관리를 맡게되었다. 팀 전체가 활용할 수 있는 공통 개발 환경을 기반을 마련하기 위해 아래와 같은 작업 수행
1. *DB 설계 및 관리*
2. *공용 DB 세팅* : 비용 절감을 위해 초기에는 Railway에 배포 후 RDS로 전환 
3. *폴더 구조 및 계층화 규칙 정의*
4. *프로젝트(메인 백엔드, 워커 백엔드) 기타 환경 세팅*

<BR>
*❓고민 포인트 - RDB vs NoSQL*
- 프로젝트는 기능이 빠르게 추가·변경되는 특성이 있어 초기에 “NoSQL이 맞지 않을까?”라는 고민도 있었다.  
- 그러나 여러 생각을 해본 결과, **RDB 구조가 더 안정적이고 팀 상황에도 적합**했다고 판단하여 도입했다.

*✔RDB 도입 상세 이유* 
1. *도메인 복잡도의 판단*
	- 게시판으로 시작한 프로젝트이다보니 도메인이 어렵다고 생각하지 않았다.
	- 관계 구조가 명확했기 때문에 굳이 NoSQL의 장점을 활용할 필요가 없다고 판단
	  
2. *팀원의 역량과 프로젝트 유지 보수*
	- NoSQL은 자유도가 높은 대신 **스키마 구조와 데이터 정합성에 대한 팀의 합의가 매우 중요**하다.
	- 자유도가 너무 높으면 초보자가 여러 명인 상황에서는 오히려 더 혼란을 초래할 수 있다고 판단
	- 또한, 리드를 해야하는 사람이 더 익숙한 DB를 선택해야 프로젝트를 리드하기 편하다고 판단했습니다.

*❓문자열 컬럼을 TEXT로 통일한 이유* 
1. *성능 차이가 거의 없다*
	- PostgreSQL에서는 `VARCHAR(n)`과 `TEXT`의 성능 차이가 사실상 거의 없다(이전에 DB 공부할 때)
2. *글자 수 고려 시간 단축*
	- 게시글 내용이라던가, 알림 메시지 등은 글자수가 고정되어 있지 않기에 매번 적절한 길이를 고민하는게 시간 낭비라고 생각했다.
	  
3. *문자열을 인덱싱할 일이 거의 없음*
	- 문자열 전체 기반 검색이 거의 없을 것 같았다
	- 혹시나 생기더라도 그 때가서 그 부분만 바꾸면 된다고 판단(참고 : [[DevStudy/DB/Postgre's TYPE/Postgre's VARCHAR vs TEXT\|Postgre's VARCHAR vs TEXT]])

### 6.3.  행동 이벤트 비동기 추적 후 추천시스템에 반영
![행동추천시스템.png](/img/user/supporter/image/%ED%96%89%EB%8F%99%EC%B6%94%EC%B2%9C%EC%8B%9C%EC%8A%A4%ED%85%9C.png)